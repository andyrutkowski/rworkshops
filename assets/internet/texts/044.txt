Internet 2 and the next generation Internet: A realistic assessment
Clearly, the Internet is now firmly established as the major commercial and consumer pathway to electronic information resources for both historical database providers such as Dialog or LEXIS-NEXIS and for the plethora of new and old content from government and private sources flowing onto the World Wide Web. And all of the Netand Web-born electronic resources come to the users under a bewildering range of constantly-shifting economic models. Every searcher today uses the Internet routinely as a way to reach traditional sources and as a tool to search for information from the new sources. The Internet is no longer merely a place for researchers to communicate, exchange data, and obtain access to supercomputers, as was the case in its early days. It has become an integral part of the corporate and consumer mainstream. 
We have begun to hear about new developments, such as Internet 2 and the Next Generation Internet (NGI) initiative, and about a new generation of advanced, high-performance applications these new developments will empower. This article will try to describe these developments and their context, to relate them to the evolution of the current Internet, and to provide some sense of their likely impact beyond the higher education and research communities. 
The Context for Internet 2 and the NGI Initiative 
Today's commercial Internet in the United States is completing a transition from a largely government-supported system designed to serve research, higher education, and federal agency missions to a public networking infrastructure serving businesses and consumers. The federal government's role was largely eliminated in the mid-1990s when the National Science Foundation phased out NSFNET. In recent months, the last of certain support functions having to do with the management of network numbers, the domain name system, and related activities has gone to the private sector or to non-governmentally supported, not-for-profit international organizations. While the issues involved in such transfers are very important, they have little to do with the day-today user of the Internet. The private corporations that currently offer Internet services have a massive program of capitol investment, capacity expansion, consolidation, and service refinement underway that will likely continue for some years to come. Basically, the corporate sector faces the basic issues of keeping up with growing demand for connectivity, capacity, and service quality, and doing so while making a profit. Appropriately, corporate Internet providers have focused on the near-term needs of the business and consumer marketplaces. 
Major Internet Service Providers (ISPs), such as Worldcom/MCI and Sprint, have made massive capacity expansions in Internet backbone systems. In the last few years the backbones have moved from DS3 (45 MB/second) speeds to OC-3 (155 MB/second), OC-12 (622 MB/second), and above. But we must keep this growth in capacity in perspective: Huge and ever-growing numbers of users share these backbones. Still, the connections available to large corporate or research campuses have increased in speed significantly, and connections at OC-3 or above have become more commonplace. Networking speeds seen by consumers or small businesses have changed little, with the tremendous problem remaining of"last-mile" network capacity for organizations that cannot justify investments of hundreds of thousands or millions of dollars a year for high-speed connectivity. This last-mile performance bottleneck still fundamentally shapes the design and deployment of most of the applications that run on the Internet, since it defines what most end users can and cannot reasonably do. 
Most end users today work with analog modems running at speeds of less that 56K/second; a few end users, and many small businesses, reach the Net through similar dial-up lines or through marginally faster technologies, such as ISDN or slow-speed frame relay. Despite the promises of new technologies that will relieve the last-mile bottleneck, such as cable TVbased connectivity, direct broadcast satellite, or digital subscriber line (DSL) schemes, the actual rollout of high-speed, low-cost connectivity is moving relatively slowly. It will be a number of years before the typical user of the Net can use advanced, bandwidth-intensive services. Note also that as a rule, a different set of companies from the traditional ISPs has the job of solving the last mile bottleneck - cable TV companies, local telephone companies, utility companies, and other new-to-the-Net players. The capital requirements and scales of these businesses differ greatly from that of the traditional ISPs offering dial-up access to consumers and leased line connectivity to large organizations. 
Only the relatively small number of users at universities, research organizations, and large corporations have access to the Net at higher speeds most typically only about 10 MB/second Ethernet. Interestingly, we already see some new applications in development in these communities, such as some universities where every dorm room now connects at Ethernet speeds (so called "Ethernet to the pillow"). Transfer of a large amount of text, as well as the (not always legal) distribution of music in digital form, will require such bandwidth. 
For the next few years, then, the emphasis in the commercial ISP world, and in the applications developed to run over the commercial offerings, will probably work around the realities of market demand and available connectivity. The research and higher education sectors, however, eagerly want to begin exploring a new generation of advanced network applications that involve orders of magnitude more end-to-end bandwidth than the current network can provide, or likely can provide in the near future. They also want to explore applications that require new network services beyond the best-efforts packet delivery offered by today's networks. 
This is the strategic context within which the Internet 2 and NGI programs have developed. 
Before going into detail about these programs, let me make three additional points to correct some misapprehensions. It is a mistake to think of Internet 2 and NGI as responses to market failures -- rather they are a strategy to support advanced applications development (and supporting infrastructure technology development) not yet viable in the commercial marketplace. The results of these efforts will feed into the evolution of the commercial Internet and the applications that it supports. The existing ISPs as well as the networking and information technology providers are major participants in, and generous underwriters of, the advanced networking initiatives. 
It is also an error to think of the advanced networking efforts as programs to create a new separate set of networks for an elite group of organizations that will then disconnect from participation in the current commercial Internet. Rather, these supplementary networks will function as protected testbeds within the fabric of the existing Internet for advanced applications development. The organizations participating in Internet 2 and NGI will continue to have a major presence on the existing Internet. Indeed, only minimal commercial traffic will likely go over the experimental networks. Internet 2 and NGI participants will still need the commercial networks for e-mail, electronic commerce, access to databases, and all of the rest of the activities that they perform today. 
Finally, remember that the Internet is a global phenomenon, not a U.S. national network. This article addresses only U.S. developments. Other nations, including Canada and the European Community members, also have major high-performance networking initiatives in various stages of development. The public and private sector roles in networking vary from one nation to another and shifts take place at different rates than in the U.S. Also, with leadership from the U.S. National Science Foundation, some international efforts have begun to link the emerging high performance networks. [For details on these developments, check http://www.startap.net.] 
Internet 2: A Higher Education Initiative 
In 1996 a group of about 35 universities joined together to build a "better faster" Net. By 1997 they had formed UCAID, the University Corporation for Advanced Internet Development, to provide administration and other support for the Internet 2 project [http:/ /www.internet2.edu]. This project now has 120 members of both university and private sector sponsors. 
The major goals set out for the Internet 2 on the UCAID Web page include the following: 
"Creating and sustaining a leading edge network capability for the national research community.... the frequent congestion of [the network's] commercial replacement have deprived many faculty of the network capability needed to support world class research." 
"Directing network development efforts to enable a new generation of applications to fully exploit the capabilities of broadband networks, media integration, interactivity, real time collaboration.... This work is essential if new priorities within higher education for support of national research objectives, distance education, lifelong learning, and related efforts are to be fulfilled." 
"Rapid transfer of new network services and application to all levels of educational use and to the broader Internet community, both nationally and internationally." 
UCAID is building a mesh of highperformance connectivity among the participating organizations called Internet 2. Internet 2 involves a multitiered strategy with regional groupings of institutions establishing what they call "gigaPoPs" or gigabit point of presence (regional networks of connection points designed to tie together the high-speed Internet2 and other types of networks). These work somewhat similarly to the regional networks established in the mid-1980s to aggregate clusters of organizations for connection to national backbone services. The installation of about 10 gigaPoPs in various regions around the country set the stage for all participating universities to have a stable Internet 2 connection by the first quarter of 1999. 
Several high-speed networks will interconnect the gigaPoPs to support research in advanced applications. These include the NSF-sponsored vBNS network, running ATM (asynchronous transfer mode) service at 622 MB/second, with typical connectivity to each gigaPoP (or in some cases directly to participating institutions) at OC-3 or OC-12 speeds. In addition, the very high-speed Abilene network will serve as an additional backbone for Internet 2; this will run Internet Protocol connections over Sonnet, another high-speed network, at speeds of OC-12 and higher, using fiber provided by Quest and switches and routers from Nortel and Cisco. 
UCAID is supported by member dues and by corporate contributions. Dues run around $50,000 a year. But besides the dues needed to support the central UCAID organization, participating organizations, in order to exploit the new capabilities of Internet 2, must make massive investments in upgrading campus networking infrastructures. This will require financing by institutional funds or by grant support. 
The UCAID Internet 2 effort focuses sharply on advanced application rather than simply infrastructure improvement. UCAID employs a team of "middleware" developers and network engineers to facilitate these efforts for participants. Here are some of the goals for the Internet 2 project: The ability of researchers to collaborate in conducting experiments. 
"Virtual Proximity" to deliver education and other services such as telemedicine and remote monitoring of the environment. 
Quality of Service (QoS) to guarantee the service level required by an application. For example, a video stream requires a stable continuous path, whereas e-mail works reliably with the current routing infrastructure. 
Promote experimentation with new communications technologies. 
The development of standards to insure interoperability. 
Technology transfer to the rest of the Internet community. 
Monitoring the impact of the new technologies and applications on the higher education and Internet communities. 
The Next Generation Internet (NGI) Program 
Unlike Internet 2, which one can view as a network (or set of networks) and related technologies developed by a university consortium, NGI [http://www.ngi.gov] is a coordinated federal funding strategy that works through an array of existing federal agencies, such as NSF and ARPA (Advanced Research Projects Agency) to support advanced networking. In many ways, one can look upon NGI as a logical successor and complement to the older High Performance Computing and Communications (HPCC) program. 
President Clinton announced NGI during the fall of 1996. The program went into place in 1997 and became a high enough priority to receive mention in the 1998 State of the Union address. 
Established as a five-year program with budgeting of roughly $100 million per year, NGI essentially gets funded by Congress year by year. Nor should one consider all that $100 million as "new money"; instead, it represents money that the federal agencies earmark for NGI activities with some of it certainly involving new money. The vast majority of this money will be spent in grants to the research and higher education communities for the development of advanced networking applications and infrastructure, and for research on key networking issues such as security, scalability, reliability, and network management. 
There are three goals set out in the Administration's initiative: Connect the universities and national research labs with networks that are 100-1,000 times faster than the current Net. 
Experiment with new technologies, especially those that support realtime video, as well as those providing "testbeds" for related experiments. 
Develop new applications for health care, national security, distance education, energy, biomedical research, environmental research, and manufacturing engineering. 
The White House National Science and Technology Council's Committee on Technology, Subcommittee on Computing, Information and Communication (CIC) R&D, Large Scale Networking Working Group oversees the NGI Implementation Team coordinating the work of the initiative. 
Clearly, NGI and Internet 2 have a highly synergistic relationship and share many common goals and objectives. The goals of NGI stretch rather broader than those of Internet 2, including a direct research agenda, attention to the needs of federal agencies and national laboratories, and to applications such as national security that go beyond the interests of the higher education community. Certainly, much of the work on Internet 2 applications and some support for infrastructure will get funding through NGI dollars via agency-based funding programs. 
Recently, some have called for a major expansion in funding for advanced networking and information technology research. [See the interim report from the Presidential Committee on Information Technology at http: //www.hpcc.gov/ac/interim/.] It is unclear how these proposals will fare in the political process of competing for funds, however. 
The New Networks: Not Just Bandwidth, but Enhanced Services 
Networks currently under development to support advance applications research via Internet 2 and NGI will certainly operate much faster than today's networks - with backbones likely reaching operating speeds of gbits/second. More importantly, with an appropriate combination of local infrastructure and long-haul backbones, this network will permit reasonably large numbers of hosts to sustain traffic rates of tens to hundreds of millions of bits per second for applications. They will also offer new network services such as Quality of Service (QoS) and multicasting, which will be important in supporting advanced applications. 
As technologies, both multicasting and QoS today lie somewhere between research and large-scale, deployable engineering. In a few cases, individual ISPs on the commercial Internet have begun to offer some of these services. Both engineering and business problems pose barriers to making these technologies widely available on the Internet as Internet-level services. One of the key goals of Internet 2 and NGI is to gain more experience with these technologies on an operational basis and to explore their interaction with application requirements. 
Very briefly, QoS involves schemes by which some applications can take priority on the network, as well as ways in which the applications can request and receive guarantees of network performance (data rates, loss rates, packet delivery delays, etc). Those interested in the technical details can consult the book by Paul Ferguson & Geoff Huston, Quality of Service: Delivering QoS on the Internet and in Corporate Networks [John Wiley & Sons, New York, 1998, ISBN 0471-24358-2]. QoS will be vital in supporting time-sensitive activities such as tele-immersion, video and audio interactions for scientific collaboration or distance education, control of instrumentation and telemetry capture, and the like. Clearly, QoS also implies the need for management schemes to determine who may obtain "premium" services and on what basis, as well as the technology to actually deliver the service. 
Multicasting is important for the support of multimedia information distribution and a variety of advanced applications involving interactions among groups of sites, rather than only two endpoints. It involves a network capability that allows a network host to transmit packets destined for a group of hosts rather than a single target computer. The routers in the network take the responsibility for duplicating the packet as required to get it economically to all of its destinations. In an application where a machine transmits a video signal to a large number of receivers, the availability of multicasting can result in a massive reduction in network traffic loads. 
As well as new network services, the new system will need new software infrastructure (sometimes termed "middleware") to support advanced applications-functions such as authentication and authorization, object naming, synchronization, and the like. While these are not considered direct Internet 2 or NGI issues, since they don't rely on the high bandwidth or new network services that the experimental networks will offer, applications developers will still need the new software developments to offer ubiquitous service. Since the experimental networks will connect relatively small and homogeneous communities (in comparison to the commercial Internet), it should be possible to deploy these software services in support of applications much more rapidly, and then, as one gains experience, extend them gradually to the broader commercial network Certainly the design and development of such services very much form a part of the NGI and Internet 2 focus on facilitating advanced applications. 
The New Applications 
We have just begun to understand the shape of the new applications. Clearly, they will include a degree of interactivity so far in advance of today's applications as to represent a truly qualitative shift-shared collaborative tele-immersion environments, for example. Distributed simulations will become commonplace. The way large databases are treated is likely to change: For example, large- scale data mining, involving massive movements of database contents across the Net, will become an active area of experimentation in the new research nets. Further, the new environment will place video, audio, computer-generated graphics, geospatial data, and high-resolution images on a much more equal footing with traditional textual content -- creating enormous problems about how to store, index, retrieve, browse, and manage this new content. The coupling of the research networks with new developments in low-cost sensor and video capture technologies should constitute a particularly powerful combination. 
Visually striking applications will likely get the most press attention and publicity, since the environment lends itself well to supporting them and they are easy to illustrate. But one should recognize that beyond these applications we will probably see some fundamental reconsideration of assumptions about how to manage data and computational resources and also considerable renewed attention on the issues involved in organizing and providing access to multimedia content. A less obvious consequence will probably include considerable evolution in thinking about authoring and reading practices for genres of multimedia works, as they become more useable within the research community. 
It is difficult to generalize about applications at this early stage: Many will develop in response to the specific needs and interests of various research communities. The Internet 2 site [http://www.internet2.edu) contains descriptions of a number of the projects currently underway. Another excellent source for a view of how the next generation of networking may reshape scientific computation is The GRID: Blueprint of a New Computing Infrastructure, edited by Ian Foster and Carl Kesselman [Morgan Kaufmann Publishers, San Francisco, 1999 ISBN 1-55860-475-81. 
Conclusions 
For those not directly involved in the development and use of the advanced applications hosted on Internet 2 and other research networks, what do these developments promise? 
1. The technologies developed here will, in probably a three to five year time frame, migrate to the commercial Internet. Migration will come in a gradual and piecemeal process. Some technologies may move faster than others. 
2. The advanced networks will provide the first serious testbed for massive libraries of digital multimedia. This content will then begin to shift to the broader commercial and consumer marketplaces, as the last mile bandwidth crunch eases. Almost everything about this kind of content, from an information management and retrieval perspective, is poorly explored. Lots of work is needed there. 
3. Data mining may begin to occur in a serious way outside of individual organizations mining organizational resource databases. Sholom M. Weiss and Nitin Indurkhya in Predictive Data Mining: A Practical Guide (Morgan Kaufman, San Francisco, 1998 155860-403-0) define data mining as "the search for valuable information in large volumes of data. It is a cooperative effort of humans and computers. Humans design databases, describe problems, and set goals. Computers sift through data, looking for patterns that match these goals." Imagine the implications of "public" data mining and of repositories and tools set up for this on the life of the professional searcher. 
4. Research networks will mainly focus on scientific, research, and education applications. Just as we saw a huge development of commercial and consumer applications in the World Wide Web as that technology moved out of the research labs and into the mainstream, we will see a similar flourishing of commercial and consumer applications as the underlying technologies of Internet 2 and NGI migrate to the commercial networks. 
Acknowledgments: My continuing gratitude to Cliff Lynch for getting me into this area and his continuing contribution to my understanding of these technologies. 
Word count: 3537
Copyright Information Today, Inc. Jan 1999

